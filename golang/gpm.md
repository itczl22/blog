### 内核线程、用户线程
<br>

__内核线程__
* 内核线程 -> CPU

* 其实等同于单线程的进程

* 内核线程的调度由操作系统负责, 一个内核线程处于阻塞状态时不影响其他的内核线程, 因为其是调度的基本单位

* 这些线程可以在全系统内进行资源的竞争

* 优点是在多处理器系统中, 内核能够同时调度同一进程中多个线程并行执行到多个处理器中; 如果进程中的一个线程被阻塞, 内核可以调度同一个进程中的另一个线程

* 缺点是即使CPU在同一个进程的多个线程之间切换, 也需要陷入内核, 因此其速度和效率不如用户级线程

__用户线程__
* 用户线程 -> 内核线程 -> CPU

* linux 的 pthread

* 用户级线程内核的切换由用户态程序自己控制(通过系统调用来获得内核提供的服务), 不需要内核干涉, 少了进出内核态的消耗

* 每个用户线程并不具有自身的线程上下文, 因此就线程的同时执行而言, 任意给定时刻每个进程只能够有一个线程在运行, 而且只有一个处理器内核会被分配给该进程, 但不能很好的利用多核cpu

* 内核资源的分配仍然是按照进程进行分配的, 各个用户线程只能在进程内进行资源竞争  

* 优点是线程的切换无需陷入内核, 故切换开销小, 速度非常快

* 缺点是系统调用的阻塞问题, 对应用程序来讲同一进程中只能同时有一个线程在运行, 一个线程的阻塞将导致整个进程中所有线程的阻塞; 由于这里的处理器时间片分配是以进程为基本单位, 所以每个线程执行的时间相对减少

__用户线程 和 内核线程 的关系 及映射模型__

* 1:1 模型  
一个用户线程就只在一个内核线程上跑, 这时可以利用多核, 进程中的一个线程阻塞, 其他线程还可以运行. 但是上下文switch很慢, 因为他要陷入内核态. linux的clone创建的就是这种模型

* N:1 模型  
多对一的模型将多个用户线程映射到一个内核线程. 多对一模型线程的切换速度要快很多, 他不需要内核态的切换(线程之间的切换由用户代码来执行). linux的pthread就是这种模型

* N:M 模型  
将多个用户线程映射到少数但不只一个内核线程中去. 多对多模型对用户线程的数量没有什么限制, 在多处理器系统上也会有一定的性能提升, 尤其是在大量用户线程存在的情况下性能提升比较显著. goroutine就是这种模型, 他通过 P 把 N:1 映射为 N:M  


#### goroutine 和 os thread
<br/>

调度
* 内核线程是有操作系统调度的, 在内核线程为cpu核数时, 执行效率最高, 因为所有的线程都在运行, 而且不需要切换. 当然了这种情况显然是不存在的

* goroutine的调度是由go runtime负责的, 他的M一般为cpu的核数, 也就是内核线程的个数等于cpu的核数

内存开销  

* 创建一个 goroutine 的栈内存消耗为 2 KB，实际运行过程中，如果栈空间不够用，会自动进行扩容  

* 创建一个 thread 则需要消耗 1 MB 栈内存，而且还需要一个被称为 “a guard page” 的区域用于和其他 thread 的栈空间进行隔离。

创建和销毁  

* goroutine 用户级的, 因为是由 Go runtime 负责管理的，创建和销毁的消耗非常小

* Thread 内核级的, 创建和销毀都会有巨大的消耗, 因为要和操作系统打交道, 通常解决的办法就是线程池  

切换  

* 每次一个线程发生切换, 都需要保存/恢复所有寄存器, 包括16个通用寄存器、PC(程序计数器）、SP（栈指针）、段寄存器(segment register)、BP(base point)、16个XMM寄存器、FP协处理器状态、X AVX寄存器以及所有MSR等  

* 而goroutine切换时, 只需要保存/恢复三个寄存器，分别是PC、SP和BP  

* go调度器和任何现代操作系统的调度器都是O(1)复杂度的, 这意味着增加线程/goroutines的数量不会增加切换时间, 但改变寄存器的代价是不可忽视的

资源

* thread竞争的“CPU”资源是真实的物理CPU

* goroutine要竞争的所谓“CPU”资源就是操作系统线程, 即内核线程


#### 什么是 GMP

 * G (Goroutine): 我们所说的协程, 为用户级的轻量级线程, 每个Goroutine对象中的sched保存着其上下文信息. 每个Goroutine对应一个G结构体, G存储Goroutine的运行堆栈、状态以及任务函数, 可重用. G并非执行体, 每个G需要绑定到P才能被调度执行. goroutine stack的size默认设置为2k

 * M (Machine): 内核线程的封装, 数量对应真实的CPU数. 代表着真正执行计算的资源, 在绑定有效的P逻辑处理器后进入调度循环. 而M本身是由操作系统来调度的. runtime对goroutine的调度机制大致是从全局队列、逻辑处理器P的本地队列以及wait队列中获取G, 切换到G的执行栈上并执行G的函数, 调用goexit做清理工作并回到M, 如此反复. M并不保留G状态, 这是G可以跨M调度的基础. M的数量是不定的, 由Go Runtime调整, 为了防止创建过多OS线程导致系统调度不过来, 目前默认最大限制为10000个

 * P (Processor): 即为G和M的调度对象, 用来调度G和M之间的关联关系. 对G来说, P逻辑处理器相当于CPU核, G只有绑定到P逻辑处理器才能被调度. 对M来说, P提供了相关的执行环境(context), 如内存分配状态(mcache), 任务队列(G)等, P的数量决定了系统内最大可并行的G的数量(前提: 物理CPU核数 >= P的数量), P的数量由GOMAXPROCS决定, 但是不论GOMAXPROCS设置为多大, P的数量最大为256


#### go scheduler

3、channel阻塞或network I/O情况下的调度
如果G被阻塞在某个channel操作或network I/O操作上时，G会被放置到某个wait队列中，而M会尝试运行下一个runnable的G；如果此时没有runnable的G供m运行，那么m将解绑P，并进入sleep状态。当I/O available或channel操作完成，在wait队列中的G会被唤醒，标记为runnable，放入到某P的队列中，绑定一个M继续执行。

4、system call阻塞情况下的调度
如果G被阻塞在某个system call操作上，那么不光G会阻塞，执行该G的M也会解绑P(实质是被sysmon抢走了)，与G一起进入sleep状态。如果此时有idle的M，则P与其绑定继续执行其他G；如果没有idle M，但仍然有其他G要去执行，那么就会创建一个新M。
当阻塞在syscall上的G完成syscall调用后，G会去尝试获取一个可用的P，如果没有可用的P，那么G会被标记为runnable，之前的那个sleep的M将再次进入sleep。

更关键地是，如果它们在网络输入操作、Sleep操作、Channel操作或 sync包的原语操作上阻塞了，也不会导致承载其多路复用的线程阻塞。如果一个goroutine在上述某个操作上阻塞，Go运行时会调度另外一 个goroutine。即使成千上万的Goroutine被创建了出来，如果它们阻塞在上述的某个操作上，也不会浪费系统资源。从操作系统的视角来看，你的程序的行为就像是一个事件驱动的C程序似的。

#### goroutine调度的几种情况
1. 每个p-m都有可运行的goroutine时

2. 有的p-m没有可运行的goroutine时

当然还有一个 work-stealing调度算法，当M执行了一些G后,如果它的queue为空，它会随机的选择另外一个P,从它的queue中取走一半的G到自己的queue中执行


全局队列是需要有锁参与的，效率肯定不高




参考：https://www.jianshu.com/p/5a4fc2729c17
